{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! Let's get to the good stuff! Let's start the same way as we explored last time in explore_code_for_smiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9296\n"
     ]
    }
   ],
   "source": [
    "from Drug import *\n",
    "from parse_xml import *\n",
    "from parse_xml import *\n",
    "\n",
    "loc = \"C:\\\\Users\\\\somat\\\\Downloads\\\\full database.xml\"\n",
    "_ = open(loc,encoding = \"utf8\")\n",
    "a = _.read()\n",
    "a = a.split(\"\\n\")\n",
    "\n",
    "a = separate_by_drug(a)\n",
    "\n",
    "drugs_library = []\n",
    "\n",
    "drugs_list = [create_drug_object(x) for x in a]\n",
    "small_molecules = list(filter(lambda x: x.lookup_attr(\"SMILES\") != \"No Data\", drugs_list))\n",
    "magic_n = 20\n",
    "print(len(small_molecules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remember making some functions to clean up the SMILES codes, so let's go and clean up our drugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smiles_functions import *\n",
    "unique_characters = create_unique_chars_histogram(small_molecules)\n",
    "cleaned_molecules = clean_molecules_using_histogram(small_molecules,unique_characters, n=magic_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6995\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_molecules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm trying to do an analysis of the molecule's structure vs its function. The function is given by the ATC code, while the molecule's structure is given by its SMILES. \n",
    "\n",
    "Let's start with the ATC codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587\n"
     ]
    }
   ],
   "source": [
    "cleaned_molecules = list(filter(lambda x: x.atc_code[0] != \"X\", cleaned_molecules))\n",
    "print(len(cleaned_molecules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What...? So few molecules left? Uh-oh. Looks like a lot of molecules didn't have atc codes listed... A serious (supervised) analysis is probably impossible with this little data, so I suppose I'll perform a toy example just to see if it's possible.\n",
    "\n",
    "Anyway, lets see our class distribution out of what's left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B', 42], ['L', 135], ['R', 129], ['H', 24], ['J', 183], ['S', 67], ['A', 150], ['V', 38], ['N', 307], ['M', 77], ['C', 218], ['D', 84], ['G', 78], ['P', 41]]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for mol in cleaned_molecules:\n",
    "    should_add = True\n",
    "    for i, label in enumerate(labels):\n",
    "        if label[0] == mol.atc_code[0]:\n",
    "            labels[i][1] += 1\n",
    "            should_add = False\n",
    "            break\n",
    "    if should_add:\n",
    "        labels.append([mol.atc_code[0], 0])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"N\" encompasses all nervous system agents. That seems like a good target.\n",
    "I'll do binary classification: nervous system agents vs non-nervous system agents. \n",
    "\n",
    "Firstly, I should make a function to encode SMILES. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_SMILES_as_onehot(SMILES, character_list):\n",
    "    encoded = []\n",
    "\n",
    "    forged_smiles = bracket_forge(element_detector(SMILES))\n",
    "    for char in forged_smiles:\n",
    "        blank = [0 for x in range(magic_n)]\n",
    "        for i, contained_character in enumerate(character_list):\n",
    "            if contained_character[0] == char:\n",
    "                blank[i] = 1\n",
    "                break\n",
    "        encoded.append(blank)\n",
    "    return encoded\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert every data point into features and labels. I'm also going to split the features/labels here into Nervous and Other because there will be a class imbalance. (more non-nervous than nervous). Also, let's set aside some test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_labels_N = []\n",
    "features_and_labels_Other = []\n",
    "for mol in cleaned_molecules:\n",
    "    feature = encode_SMILES_as_onehot(mol.lookup_attr(\"SMILES\"), unique_characters)\n",
    "    label = mol.atc_code[0] == \"N\"\n",
    "    label = [label, not label]\n",
    "    if label[0]:\n",
    "        features_and_labels_N.append([feature, label])\n",
    "    else:\n",
    "        features_and_labels_Other.append([feature, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "278\n",
      "1152\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(features_and_labels_N)\n",
    "random.shuffle(features_and_labels_Other)\n",
    "stop_N = int(len(features_and_labels_N)*0.1)\n",
    "stop_O = int(len(features_and_labels_Other)*0.1)\n",
    "\n",
    "features_and_labels_test = features_and_labels_N[:stop_N] + features_and_labels_Other[:stop_O]\n",
    "features_and_labels_N = features_and_labels_N[stop_N:]\n",
    "features_and_labels_Other = features_and_labels_Other[stop_O:]\n",
    "\n",
    "print(len(features_and_labels_test))\n",
    "print(len(features_and_labels_N))\n",
    "print(len(features_and_labels_Other))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's all we've got. Let's make it count! and build the RNN.\n",
    "\n",
    "Actually, first, we have to pad each one-hot to the longest length in the batch. Let's make a function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_onehots(onehot_batch):\n",
    "    o = onehot_batch\n",
    "    onehot_lengths = [len(x) for x in onehot_batch]\n",
    "    maximum_length = max(onehot_lengths)\n",
    "    for i, _ in enumerate(o):\n",
    "        while len(o[i]) < maximum_length:\n",
    "            o[i].append([0 for x in range(magic_n)])\n",
    "    return o, onehot_lengths\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess= tf.Session()\n",
    "features = tf.placeholder(tf.float32, [None, None, magic_n])\n",
    "labels = tf.placeholder(tf.float32, [None, 2])\n",
    "seq_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "layers_f = [tf.contrib.rnn.LSTMCell(unit) for unit in [1024,512]]\n",
    "stacked_f = tf.contrib.rnn.MultiRNNCell(layers_f)\n",
    "\n",
    "LSTM_output,_ = tf.nn.dynamic_rnn(stacked_f,features,dtype = tf.float32,sequence_length = seq_lengths)\n",
    "indices = tf.stack([tf.range(tf.shape(LSTM_output)[0]),seq_lengths-1], axis=1)\n",
    "last_output = tf.gather_nd(LSTM_output, indices)\n",
    "\n",
    "hidden_layer = tf.layers.dense(inputs = last_output, units = 256, activation = tf.nn.relu)\n",
    "final_layer = tf.layers.dense(inputs = hidden_layer, units = 2, activation = tf.nn.softmax)\n",
    "\n",
    "prediction = tf.argmax(final_layer, axis=1)\n",
    "correct_preds = tf.math.equal(tf.cast(prediction,tf.int32), tf.cast(tf.argmax(labels, axis=1), tf.int32))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds,tf.float32)) / tf.cast(tf.shape(prediction)[0], tf.float32)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(labels, final_layer)\n",
    "opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this work... but will it blend?\n",
    "I mean train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc 0.45\n",
      "Train acc 0.5\n"
     ]
    }
   ],
   "source": [
    "for z in range(100000):\n",
    "    selected_samples = random.sample(features_and_labels_N, 20) + random.sample(features_and_labels_Other, 20)\n",
    "    train_features, train_seq_lengths = pad_onehots([x[0] for x in selected_samples])\n",
    "    train_labels = [x[1] for x in selected_samples]\n",
    "    acc, _ = sess.run([accuracy, opt], feed_dict = {features:train_features, labels:train_labels, seq_lengths:train_seq_lengths})\n",
    "    if z % 100 == 0:\n",
    " \n",
    "        print(\"Train acc \"+ str(acc))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
